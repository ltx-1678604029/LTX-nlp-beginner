{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be71340",
   "metadata": {},
   "source": [
    "### 任务三：基于注意力机制的文本匹配\n",
    "\n",
    "输入两个句子判断，判断它们之间的关系。参考ESIM（可以只用LSTM，忽略Tree-LSTM），用双向的注意力机制实现。\n",
    "\n",
    "#### 知识点：\n",
    "\n",
    "注意力机制\n",
    "\n",
    "token2token attetnion\n",
    "\n",
    "#### 本文暂不考虑tree-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fcfd6a",
   "metadata": {},
   "source": [
    "#### 有关LSTM：\n",
    "https://www.cnblogs.com/wangduo/p/6773601.html?utm_source=itdadao&utm_medium=referral\n",
    "#### 有关bilstm（attention）：\n",
    "https://blog.csdn.net/qq_34992900/article/details/115443992\n",
    "#### 有关RNN，attention等\n",
    "https://www.bilibili.com/video/BV15b4y1R7c9?spm_id_from=333.337.search-card.all.click\n",
    "\n",
    "（发展变化及原理初探: 神经语言模型(NNLM)、循环神经网络(RNN)和注意力(Attention)机......）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a352ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782ecd4",
   "metadata": {},
   "source": [
    "#### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ee7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field(lower=True,batch_first=True,include_lengths=True)\n",
    "LABEL=data.LabelField(batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0619088e",
   "metadata": {},
   "source": [
    "#### 在定义fields时，我们可以看到数据集中README部分：\n",
    "\n",
    "sentence1: The premise caption that was supplied to the author of the pair.\n",
    "\n",
    "sentence2: The hypothesis caption that was written by the author of the pair.\n",
    "\n",
    "sentence{1,2}_parse: The parse produced by the Stanford Parser (3.5.2, case insensitive PCFG, trained on the standard training set augmented with the parsed Brown Corpus) in Penn Treebank format.\n",
    "\n",
    "sentence{1,2}_binary_parse: The same parse as in sentence{1,2}_parse, but formatted for use in tree-structured neural networks with no unary nodes and no labels.\n",
    "\n",
    "annotator_labels (label1-5 in the tab separated file): These are all of the individual labels from annotators in phases 1 and 2. The first label comes from the phase 1 author, and is the only label for examples that did not undergo phase 2 annotation. In a few cases, the one of the phase 2 labels may be blank, indicating that an annotator saw the example but could not annotate it.\n",
    "\n",
    "gold_label: This is the label chosen by the majority of annotators. Where no majority exists, this is '-', and the pair should not be included when evaluating hard classification accuracy.\n",
    "\n",
    "captionID: A unique identifier for each sentence1 from the original Flickr30k example.\n",
    "\n",
    "pairID: A unique identifier for each sentence1--sentence2 pair.\n",
    "\n",
    "NOTE: captionID and pairID contain information that can be useful in making classification decisions and should not be included in model input (nor, of course, should either annotator_labels or gold_label).\n",
    "\n",
    "#### 可以看到在我们不考虑tree-LSTM时，我们仅需利用sentence1（text），sentence2（text），gold_label（label）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2cf6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields={'sentence1':('premise',TEXT),'sentence2':('hypothesis',TEXT),'gold_label':('label',LABEL)}\n",
    "\n",
    "train_data,dev_data,test_data=data.TabularDataset.splits(\n",
    "    path=r'C:\\Users\\16786\\Desktop\\复旦nlp上手教程\\task3\\data',\n",
    "    train='snli_1.0_train.jsonl',\n",
    "    validation='snli_1.0_dev.jsonl',\n",
    "    test='snli_1.0_test.jsonl',\n",
    "    format='json',\n",
    "    fields=fields,\n",
    "    filter_pred=lambda x:x.label!='-'#筛选出标签为'-'（即无标签）的例子。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b95925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "vectors=Vectors('glove.6B.300d.txt',r'C:\\Users\\16786\\Desktop\\复旦nlp上手教程\\task3\\GLOVE')\n",
    "\n",
    "TEXT.build_vocab(train_data,vectors=vectors,unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd3b62",
   "metadata": {},
   "source": [
    "注：Tensor.normal_(mean=0, std=1, *, generator=None) → Tensor\n",
    "\n",
    "Fills tensor with elements samples from the normal distribution parameterized by mean and std.self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab1e5c",
   "metadata": {},
   "source": [
    "##### 迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b106b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "batch=32\n",
    "device=\"cpu\"\n",
    "\n",
    "train_iter,dev_iter=BucketIterator.splits(\n",
    "    (train_data,dev_data),\n",
    "    batch_size=batch,\n",
    "    device=device,\n",
    "    sort_key=lambda x:len(x.premise)+len(x.hypothesis),\n",
    "    sort_within_batch=True,\n",
    "    repeat=False,\n",
    "    shuffle=True\n",
    ")\n",
    "test_iter=data.Iterator(\n",
    "    test_data,              \n",
    "    batch_size=batch,\n",
    "    device=device,\n",
    "    sort=False,\n",
    "    sort_within_batch=False,\n",
    "    repeat=False,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6982660",
   "metadata": {},
   "source": [
    "简单检查一下迭代器能否正常使用，别写完训练发现迭代器有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c94f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch=next(iter(train_iter))\n",
    "# premise, premise_lens = batch.premise\n",
    "# print(batch.premise)\n",
    "# print(premise.size(1))\n",
    "# print(premise)\n",
    "# print(premise_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9a7d8",
   "metadata": {},
   "source": [
    "## ESIM：\n",
    "\n",
    "Input Encoding →Local Inference Modeling→ Inference Composition→Prediction（也就是pooling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193597ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class input_encoding(nn.Module):\n",
    "    def __init__(self,num_features,embedding_size,hidden_size,num_layers,vectors,batch_first=True,dropout=0.5):\n",
    "        super(input_encoding,self).__init__()\n",
    "        self.num_features=num_features\n",
    "        self.embedding_size=embedding_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.embedding=nn.Embedding.from_pretrained(vectors)    #from_pretrained加载预训练好的词向量\n",
    "        self.bilstm=nn.LSTM(input_size=embedding_size,hidden_size=hidden_size//2,num_layers=num_layers,batch_first=True,dropout=dropout,bidirectional=True)\n",
    "        # every LSTM's(forward and backward) hidden size is half of HIDDEN_SIZE\n",
    "        \n",
    "    def forward(self,x,lens):\n",
    "        a=self.embedding(x)\n",
    "        b=self.dropout(a)\n",
    "        c,_=self.bilstm(b)\n",
    "        return c\n",
    "\n",
    "class local_inference_modeling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(local_inference_modeling,self).__init__()\n",
    "        \n",
    "        self.softmax_1=nn.Softmax(dim=1)\n",
    "        self.softmax_2=nn.Softmax(dim=2)\n",
    "    def forward(self,a,b):\n",
    "        #equation 11 in paper:\n",
    "        e=torch.matmul(a,b.transpose(1,2))#torch.matmul()类似于矩阵相乘，但可以利用python 中的广播机制，与下文.bmm()形成对比\n",
    "        #equation 12 in paper:\n",
    "        a_=(self.softmax_2(e)).bmm(b)#.bmm()矩阵相乘\n",
    "        #equation 13 in paper:\n",
    "        b_=(self.softmax_1(e).transpose(1, 2)).bmm(a)\n",
    "        #equation 14 in paper:\n",
    "        m_a=torch.cat([a,a_,a-a_,a*a_],dim=-1)#按行（横着）拼接\n",
    "        #equation 15 in paper:\n",
    "        m_b=torch.cat([b,b_,b-b_,b*b_],dim=-1)\n",
    "        return m_a,m_b\n",
    "    \n",
    "class inference_composition(nn.Module):\n",
    "    def __init__(self,num_features,input_size,hidden_size,num_layers,embedding_size,batch_first=True,dropout=0.5):\n",
    "        super(inference_composition,self).__init__()\n",
    "        self.linear=nn.Linear(4*hidden_size,hidden_size)#4*hiddensize=inputsize\n",
    "        self.bilstm=nn.LSTM(input_size=hidden_size,hidden_size=hidden_size//2,num_layers=num_layers,dropout=dropout,bidirectional=True)\n",
    "        #！！！！！！！！！！这里hidden_size一定要记得//2！！！！！！！！\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,lens):\n",
    "        a=self.linear(x)\n",
    "        b=self.dropout(a)\n",
    "        c,_=self.bilstm(b)\n",
    "        return c   \n",
    "\n",
    "class prediction(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_classes=4, dropout=0.5):\n",
    "        super(prediction, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size,output_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(output_size,num_classes)\n",
    "        )\n",
    "    def forward(self,a,b):\n",
    "        #equation 18,19 in paper:\n",
    "        v_a_avg=F.avg_pool1d(a.transpose(1,2),a.size(1)).squeeze(-1)#.size(1) 类似“长度”\n",
    "        v_a_max=F.max_pool1d(a.transpose(1,2),a.size(1)).squeeze(-1)\n",
    "        v_b_avg=F.avg_pool1d(b.transpose(1,2),b.size(1)).squeeze(-1)\n",
    "        v_b_max=F.max_pool1d(b.transpose(1,2),b.size(1)).squeeze(-1)\n",
    "        #equation 20 in paper:\n",
    "        out=torch.cat((v_a_avg,v_a_max,v_b_avg,v_b_max),dim=-1)\n",
    "        output=self.mlp(out)\n",
    "        return output\n",
    "\n",
    "class ESIM(nn.Module):\n",
    "    def __init__(self,num_features,hidden_size,embedding_size,num_classes=4,vectors=None,num_layers=1,batch_first=True,drop_out=0.5,freeze=False):\n",
    "        super(ESIM,self).__init__()\n",
    "        self.embedding_size=embedding_size\n",
    "        self.input_encoding=input_encoding(num_features,embedding_size,hidden_size,num_layers,vectors,dropout=0.5,batch_first=True)\n",
    "        self.local_inference_modeling=local_inference_modeling()\n",
    "        self.inference_composition=inference_composition(num_features,4*hidden_size,hidden_size,num_layers,embedding_size,batch_first=True,dropout=0.5)\n",
    "        self.prediction=prediction(4*hidden_size,hidden_size,num_classes,drop_out)\n",
    "    def forward(self,a,len_a,b,len_b):\n",
    "        a_bar=self.input_encoding(a,len_a)\n",
    "        b_bar=self.input_encoding(b,len_b)\n",
    "        m_a,m_b=self.local_inference_modeling(a_bar,b_bar)\n",
    "        v_a=self.inference_composition(m_a,len_a)\n",
    "        v_b=self.inference_composition(m_b,len_b)\n",
    "        out_put=self.prediction(v_a,v_b)\n",
    "        return out_put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77024849",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=600\n",
    "epochs=20\n",
    "dropout=0.5\n",
    "num_layers=1\n",
    "learning_rate=4e-4\n",
    "embedding_size=300\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e171c6c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16786\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = ESIM(num_features=(TEXT.vocab),hidden_size=hidden_size,\n",
    "             embedding_size=embedding_size,num_classes=4,\n",
    "             vectors=TEXT.vocab.vectors,num_layers=num_layers,\n",
    "             batch_first=True, drop_out=0.5, freeze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217fa38",
   "metadata": {},
   "source": [
    "### train\n",
    "有关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_iter,dev_iter,loss_func,optimizer,epochs):\n",
    "    best_acc=-1\n",
    "    patience_count=0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        n=0\n",
    "        for batch in tqdm(train_iter):\n",
    "            premise,premise_lens=batch.premise\n",
    "            hypothesis,hypothesis_lens=batch.hypothesis\n",
    "            labels=batch.label\n",
    "\n",
    "            model.zero_grad()\n",
    "            output=model(premise,premise_lens,hypothesis, hypothesis_lens).to(device)\n",
    "            loss=loss_func(output,labels)\n",
    "            total_loss+=loss.item()\n",
    "            n+=batch_size\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if n%3600==0:\n",
    "                print('epoch : {} step : {}, loss : {}'.format(epoch,int(n/3600),total_loss/n))\n",
    "        tqdm.write(\"Epoch: %d, Train Loss: %d\"%(epoch+1,total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e0218ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/17168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 6,493,204 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                           | 225/17168 [02:24<3:07:22,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 2, loss : 0.033685843124985695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▉                                                                          | 450/17168 [04:53<2:53:30,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 4, loss : 0.03283083680189318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                         | 675/17168 [07:23<3:37:28,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 6, loss : 0.03207162418023304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▉                                                                        | 900/17168 [09:57<3:15:45,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 8, loss : 0.03161189170761241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|████▉                                                                      | 1125/17168 [12:33<4:16:34,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 step : 10, loss : 0.03129819345143106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████                                                                      | 1170/17168 [13:06<2:59:11,  1.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b40c4914c843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The model has {count_parameters(model):,} trainable parameters'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-f4d17c0890fe>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, dev_iter, loss_func, optimizer, epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mn\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;31m#torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "train(train_iter, dev_iter, loss_func, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08dd8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3d008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b357df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830b68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f06958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
